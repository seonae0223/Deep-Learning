{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMNOoEzX2SXIBtSR7Xu6jVC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seonae0223/Deep_Learning/blob/main/04_PyTorch_Model_Training_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # 데이터를 저장할 root 디렉토리\n",
        "    train=True, # 훈련용 데이터 설정\n",
        "    download=True, # 다운로드\n",
        "    transform=ToTensor() # 이미지 변환. 여기서는 TorchTesnor로 변환시킵니다.\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "4eZ3gu_nziwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_dataloader  = DataLoader(test_data, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "vuyVlrbjzlqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    # nn.Sequential을 이용해 연속되는 레이어의 구조를 구성\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.2),\n",
        "        nn.Linear(128, 10),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    y = self.linear_relu_stack(x)\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "KMYDtfX30Ez0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_8vJK3x1NnY",
        "outputId": "c7225487-dc8d-4b11-c93f-62130f687580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.2, inplace=False)\n",
            "    (3): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 훈련\n",
        "\n",
        "파이토치의 모델 훈련을 위해서는 손실함수(Loss/모델의 성능), 최적화 함수(Optimizer)를 등록해야 합니다. 특히 최적화 함수를 사용하기 위해서는 `model.parameters()` 메소드를 이용해 최적화 대상 파라미터(가중치, bias)를 지정해주면 됩니다."
      ],
      "metadata": {
        "id": "pTJBpCZH0MAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ⭐아주 주요한 부분\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer : 경사하강법을 수행하기 위한 함수. 경사하강법은 어디에 수행? W(가중치), b(bias) -> parameters\n",
        "# model에서 파라미터를 꺼내서 최적화 함수에 등록\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "qLUaPPH90GMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 과정(훈련 루프 정의)\n",
        "# 1. dataloader에서 데이터를 꺼낸다.\n",
        "# 2. 데이터를 모델에 통과시킨다. (순전파를 통한 추론 - prediction(inference))\n",
        "# 3. 얻어낸 예측값을 이용해서 loss를 계산\n",
        "# 4. 역전파를 통한 미분값을 계산\n",
        "# 5. 얻어낸 미분 값으로 경사하강법을 수행(최적화)\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  # 데이터 로더에 있는 데이터 세트의 길이 가져오기\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "\n",
        "  # 중요! model을 모드로 설정.⭐️\n",
        "  model.train()\n",
        "  # 데이터 꺼내기\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # 현재 데이터 로더에 있는 데이터는 cpu에 존재하고 있기 때문에 이 데이터들을 GPU로 옮긴다.\n",
        "    #   모델이 위치한 곳과 데이터가 위치한 곳을 동일하게 맞춰준다.\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # 순전파 수행\n",
        "    pred = model(X)\n",
        "\n",
        "    # 손실 계산\n",
        "    loss = loss_fn(pred, y) # 자동으로 소프트맥스가 적용됨.\n",
        "\n",
        "    # 역전파 수행(미분값 없어내기)\n",
        "    optimizer.zero_grad() # 기존에 남아있던 기울기를 제거(이전 배치의 기울기가 남아있으면 정확한 기울기를 구해내기가 힘듦)\n",
        "    loss.backward() # 역전파. loss가 Leaf\n",
        "    optimizer.step() # 구한 미분값을 토대로 최적화를 수행(경사하강법)\n",
        "\n",
        "    # 배치가 100번 돌 때마다 화면에 출력\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"Train Loss : {loss:>7f} [ {current:>5d} / {size:>5d} ]\")"
      ],
      "metadata": {
        "id": "cIEJSmXM1BP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 추론을 위한 테스트 과정(테스트 루프) 정의\n",
        "# 1. 테스트 데이터 로더에서 데이터 꺼내기\n",
        "# 2. 데이터를 모델에 통과(순전파)시켜서 예측값 얻어내기\n",
        "# 3. 성능(metric) 계산. - Loss, Accuracy 계산\n",
        "#   - 배치 별 평균 성능 계산\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  # 중요! 평가모드(추론모드) 설정\n",
        "  model.eval()\n",
        "\n",
        "  # 추론 과정은 기울기를 구할 필요가 없어요\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "\n",
        "      # 모델과 데이터는 항상 같은 환경에서 사용되어야 한다.\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # 예측\n",
        "      pred = model(X)\n",
        "\n",
        "      # Loss 계산\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "\n",
        "      # 맞춘거 개수 합치기\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  # 배치 개수 구하기\n",
        "  num_batches = len(dataloader)\n",
        "\n",
        "  # 배치 별 loss, accuracy의 평균 구하기\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "\n",
        "  print(f\"Test Error : \\n Accuracy : {(100*correct):>0.1f}%, Avg Loss : {test_loss:>8f}\\n\")\n"
      ],
      "metadata": {
        "id": "WPROAcMg5uSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 훈련 수행"
      ],
      "metadata": {
        "id": "1gjT4ntc-Jen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "for i in range(epochs):\n",
        "  print(f\"Epochs {i + 1}\\n......................\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model, loss_fn)\n",
        "\n",
        "print(\"Done!!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V4pKuly-IZW",
        "outputId": "46dcadd7-6ef0-4cf7-a2d9-136e9a09987a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs 1\n",
            "......................\n",
            "Train Loss : 2.195719 [     0 / 60000 ]\n",
            "Train Loss : 2.213618 [  6400 / 60000 ]\n",
            "Train Loss : 2.184967 [ 12800 / 60000 ]\n",
            "Train Loss : 2.193613 [ 19200 / 60000 ]\n",
            "Train Loss : 2.199875 [ 25600 / 60000 ]\n",
            "Train Loss : 2.187859 [ 32000 / 60000 ]\n",
            "Train Loss : 2.189427 [ 38400 / 60000 ]\n",
            "Train Loss : 2.188310 [ 44800 / 60000 ]\n",
            "Train Loss : 2.186734 [ 51200 / 60000 ]\n",
            "Train Loss : 2.163091 [ 57600 / 60000 ]\n",
            "Test Error : \n",
            " Accuracy : 28.2%, Avg Loss : 2.165968\n",
            "\n",
            "Epochs 2\n",
            "......................\n",
            "Train Loss : 2.166116 [     0 / 60000 ]\n",
            "Train Loss : 2.144289 [  6400 / 60000 ]\n",
            "Train Loss : 2.158753 [ 12800 / 60000 ]\n",
            "Train Loss : 2.120757 [ 19200 / 60000 ]\n",
            "Train Loss : 2.136964 [ 25600 / 60000 ]\n",
            "Train Loss : 2.152539 [ 32000 / 60000 ]\n",
            "Train Loss : 2.117127 [ 38400 / 60000 ]\n",
            "Train Loss : 2.159462 [ 44800 / 60000 ]\n",
            "Train Loss : 2.118912 [ 51200 / 60000 ]\n",
            "Train Loss : 2.148128 [ 57600 / 60000 ]\n",
            "Test Error : \n",
            " Accuracy : 36.9%, Avg Loss : 2.123652\n",
            "\n",
            "Epochs 3\n",
            "......................\n",
            "Train Loss : 2.112446 [     0 / 60000 ]\n",
            "Train Loss : 2.129731 [  6400 / 60000 ]\n",
            "Train Loss : 2.112391 [ 12800 / 60000 ]\n",
            "Train Loss : 2.106288 [ 19200 / 60000 ]\n",
            "Train Loss : 2.167122 [ 25600 / 60000 ]\n",
            "Train Loss : 2.145971 [ 32000 / 60000 ]\n",
            "Train Loss : 2.125072 [ 38400 / 60000 ]\n",
            "Train Loss : 2.090577 [ 44800 / 60000 ]\n",
            "Train Loss : 2.099082 [ 51200 / 60000 ]\n",
            "Train Loss : 2.075733 [ 57600 / 60000 ]\n",
            "Test Error : \n",
            " Accuracy : 44.4%, Avg Loss : 2.082194\n",
            "\n",
            "Epochs 4\n",
            "......................\n",
            "Train Loss : 2.100547 [     0 / 60000 ]\n",
            "Train Loss : 2.084792 [  6400 / 60000 ]\n",
            "Train Loss : 2.050725 [ 12800 / 60000 ]\n",
            "Train Loss : 2.070418 [ 19200 / 60000 ]\n",
            "Train Loss : 2.038761 [ 25600 / 60000 ]\n",
            "Train Loss : 2.047262 [ 32000 / 60000 ]\n",
            "Train Loss : 2.055721 [ 38400 / 60000 ]\n",
            "Train Loss : 2.026744 [ 44800 / 60000 ]\n",
            "Train Loss : 2.049101 [ 51200 / 60000 ]\n",
            "Train Loss : 2.007460 [ 57600 / 60000 ]\n",
            "Test Error : \n",
            " Accuracy : 49.4%, Avg Loss : 2.040850\n",
            "\n",
            "Epochs 5\n",
            "......................\n",
            "Train Loss : 2.059625 [     0 / 60000 ]\n",
            "Train Loss : 2.016398 [  6400 / 60000 ]\n",
            "Train Loss : 2.061698 [ 12800 / 60000 ]\n",
            "Train Loss : 2.010730 [ 19200 / 60000 ]\n",
            "Train Loss : 2.062405 [ 25600 / 60000 ]\n",
            "Train Loss : 2.069403 [ 32000 / 60000 ]\n",
            "Train Loss : 1.988526 [ 38400 / 60000 ]\n",
            "Train Loss : 2.025430 [ 44800 / 60000 ]\n",
            "Train Loss : 2.012225 [ 51200 / 60000 ]\n",
            "Train Loss : 1.994738 [ 57600 / 60000 ]\n",
            "Test Error : \n",
            " Accuracy : 52.5%, Avg Loss : 1.999215\n",
            "\n",
            "Epochs 6\n",
            "......................\n",
            "Train Loss : 2.008425 [     0 / 60000 ]\n",
            "Train Loss : 1.979069 [  6400 / 60000 ]\n",
            "Train Loss : 1.984371 [ 12800 / 60000 ]\n",
            "Train Loss : 1.982118 [ 19200 / 60000 ]\n",
            "Train Loss : 1.954846 [ 25600 / 60000 ]\n",
            "Train Loss : 1.981826 [ 32000 / 60000 ]\n",
            "Train Loss : 2.030626 [ 38400 / 60000 ]\n",
            "Train Loss : 1.974594 [ 44800 / 60000 ]\n",
            "Train Loss : 2.000755 [ 51200 / 60000 ]\n",
            "Train Loss : 1.960901 [ 57600 / 60000 ]\n",
            "Test Error : \n",
            " Accuracy : 54.6%, Avg Loss : 1.957353\n",
            "\n",
            "Epochs 7\n",
            "......................\n",
            "Train Loss : 1.934646 [     0 / 60000 ]\n",
            "Train Loss : 1.983697 [  6400 / 60000 ]\n",
            "Train Loss : 1.914773 [ 12800 / 60000 ]\n",
            "Train Loss : 1.924595 [ 19200 / 60000 ]\n",
            "Train Loss : 1.917448 [ 25600 / 60000 ]\n",
            "Train Loss : 1.946027 [ 32000 / 60000 ]\n",
            "Train Loss : 2.011386 [ 38400 / 60000 ]\n",
            "Train Loss : 1.973663 [ 44800 / 60000 ]\n",
            "Train Loss : 1.907969 [ 51200 / 60000 ]\n",
            "Train Loss : 1.922407 [ 57600 / 60000 ]\n",
            "Test Error : \n",
            " Accuracy : 56.6%, Avg Loss : 1.915126\n",
            "\n",
            "Epochs 8\n",
            "......................\n",
            "Train Loss : 1.891472 [     0 / 60000 ]\n",
            "Train Loss : 1.913495 [  6400 / 60000 ]\n",
            "Train Loss : 1.940630 [ 12800 / 60000 ]\n",
            "Train Loss : 1.918645 [ 19200 / 60000 ]\n",
            "Train Loss : 1.877504 [ 25600 / 60000 ]\n",
            "Train Loss : 1.912158 [ 32000 / 60000 ]\n",
            "Train Loss : 1.901081 [ 38400 / 60000 ]\n",
            "Train Loss : 1.856568 [ 44800 / 60000 ]\n",
            "Train Loss : 1.857530 [ 51200 / 60000 ]\n",
            "Train Loss : 1.838493 [ 57600 / 60000 ]\n",
            "Test Error : \n",
            " Accuracy : 58.0%, Avg Loss : 1.872646\n",
            "\n",
            "Epochs 9\n",
            "......................\n",
            "Train Loss : 1.854764 [     0 / 60000 ]\n",
            "Train Loss : 1.865323 [  6400 / 60000 ]\n",
            "Train Loss : 1.860057 [ 12800 / 60000 ]\n",
            "Train Loss : 1.815359 [ 19200 / 60000 ]\n",
            "Train Loss : 1.915175 [ 25600 / 60000 ]\n",
            "Train Loss : 1.817253 [ 32000 / 60000 ]\n",
            "Train Loss : 1.853022 [ 38400 / 60000 ]\n",
            "Train Loss : 1.891106 [ 44800 / 60000 ]\n",
            "Train Loss : 1.855572 [ 51200 / 60000 ]\n",
            "Train Loss : 1.789675 [ 57600 / 60000 ]\n",
            "Test Error : \n",
            " Accuracy : 59.1%, Avg Loss : 1.830200\n",
            "\n",
            "Epochs 10\n",
            "......................\n",
            "Train Loss : 1.854499 [     0 / 60000 ]\n",
            "Train Loss : 1.839347 [  6400 / 60000 ]\n",
            "Train Loss : 1.791198 [ 12800 / 60000 ]\n",
            "Train Loss : 1.836210 [ 19200 / 60000 ]\n",
            "Train Loss : 1.774601 [ 25600 / 60000 ]\n",
            "Train Loss : 1.778655 [ 32000 / 60000 ]\n",
            "Train Loss : 1.757180 [ 38400 / 60000 ]\n",
            "Train Loss : 1.824858 [ 44800 / 60000 ]\n",
            "Train Loss : 1.837469 [ 51200 / 60000 ]\n",
            "Train Loss : 1.786374 [ 57600 / 60000 ]\n",
            "Test Error : \n",
            " Accuracy : 60.0%, Avg Loss : 1.788267\n",
            "\n",
            "Done!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AlcMWiN1-fdc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}